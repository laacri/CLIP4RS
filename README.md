# Benchmarking CLIP Models for Remote Sensing Applications

### Abstract
This thesis investigates the application and adaptation of CLIP-like Vision-Language Models (VLMs) for Remote Sensing (RS) classification tasks, with a focus on incorporating multispectral data. While CLIP is well known for its remarkable zero-shot capabilities on standard images, its use in RS has been limited by the domain gap and the RGB constraint of its pretrained image encoder. To address these limitations, we propose a lightweight, modular embedder that projects multispectral satellite imagery onto a format compatible with CLIPâ€™s image encoder. We evaluate this approach across seven datasets, including EuroSAT and the GEO-Bench benchmark, spanning binary, multiclass, and multilabel classification tasks. Our findings show that, even without retraining CLIP or applying prompt tuning, the proposed MSI + CLIP models consistently improve upon zero-shot baselines in task-specific training scenarios. We also explore transfer learning across datasets, highlighting the limitations introduced by resolution gaps, spectral mismatches, and semantic differences. While our approach struggles with multilabel classification, we outline promising future directions based on recent advances in multimodal learning. Overall, this work offers a modular and efficient path toward enhancing CLIP for Earth Observation, demonstrating that multispectral integration can substantially boost model performance with minimal architectural overhead.
